---
title: "Engineering large-scale geospatial rasters with xarray and dask"
author: "Clinton Oyogo"
format: 
  revealjs:
    scrollable: true
    smaller: true  
editor: visual
---

## Agenda {.smaller}

-   Definitions
-   Spatial data
-   Background
-   Solution architecture
-   Critical design decisions
-   Best practises summary
-   Further reading

## Definitions {.smaller}

:::: columns
::: {.column width="50%"}
-   _Geospatial data:_ spatially referenced datasets, typically vector or raster.\
-   _Vector data:_ points, lines, polygons (e.g adminstrative boundaries, roads, buildings).\
-   _CRS (Coordinate Reference System):_ How coordinates map to Earth's surface.
-   _Resolution:_ size of each pixel (e.g 100m by 100m).\
-   _Small area estimation:_ a statistical technique used to produce estimates for small geographic areas or domains when direct estimates from surveys are unreliable due to small sample sizes.
:::

::: {.column width="50%"}

```         
A raster is a 2D matrix where each cell has:
- Geographic coordinates (latitude/longitude or projected)
- A value (temperature, population count, land cover class)
- Metadata (CRS, resolution, extent, nodata values)

Example: Population density raster
┌────┬────┬────┬────┐
│ 50 │120 │ 80 │ 30 │  ← Each cell=people per km²
├────┼────┼────┼────┤
│ 90 │200 │150 │ 60 │
├────┼────┼────┼────┤
│ 40 │100 │ 90 │ 45 │
└────┴────┴────┴────┘
```
![](www/raster.png)
:::
::::

## Spatial data {.smaller}

:::: columns
::: {.column width="50%"}

::: {.callout-tip icon=false}
### Benefits
-   Satellite imagery is gaining increasing attention as an unconventional data source for socioeconomic research due to its consistency, timeliness, and scalability.  
-   Researchers are using satellite data to estimate poverty levels, analyze urbanization trends, and assess agricultural productivity and a myriad other use cases.  
:::

:::

::: {.column width="50%" style="color: maroon"}

::: {.callout-warning icon=false}
### Bottlenecks

-   While satellite data has gained popularity, working with spatial data often presents challenges.
-   Its not structured in tabular format, therefore, requiring specialized processing techniques, and...
-   can be extremely large in size necessitating advanced methods for efficient processing.

:::


:::
::::

## Background: Small Area Estimation {.smaller}

-   As part of a small area estimation workflow, we needed to extract zonal statistics from geospatial data for use in modelling poverty estimates at granular levels.
-   We had to get data from various sources, harmonise the raster files, and then extract zonal statistics at the lowest adminstrative level.\
-   While this was feasible for Bangladesh, one of the project focus countries, it quickly became clear that it might not be scalable for other project countries due to their large sizes and hence massive raster datasets.
-   For instance, we worked with six population density raster from Meta, and their sizes varied significantly based on country sizes, ranging from 500MBs to 20GB per file.
-   Processing 6 such files alongside an additional 20 raster files for other variables posed significant computational challenges. And this is what led me to investigate ways to process huge raster files within limited resources.

## Solution architecture: *Xarray* {.smaller}

### Why Xarray?

:::: {.columns}

::: {.column width="50%"}

![](www/xarray.png) 

-   Xarray provides N-dimensional labeled arrays with built-in geospatial awareness.
    -   Labeled dimensions: work with coordinates, not array indices.
    -   Lazy evaluation: data loaded only when needed.
    -   Integration with Dask: seamless parallel processing.
::: 

::: {.column width="50%"}  


```{.python code-line-numbers="|1-3|5-11"}
# Traditional NumPy: position-based, no metadata
raster = np.array([...])  # What CRS? What coordinates? 
value = raster[100, 200]  # What location is this?

# Xarray: labeled, self-describing
raster = xr.DataArray(
    data=np.array([...]),
    dims=("y", "x"),
    coords={"y": [...], "x": [...]},
    attrs={"crs": "EPSG:4326", "resolution": 100}
)
value = raster.sel(y=-10.5, x=35.2)  # Select by actual coordinates!
```        

:::   

::::

## Solution architecture: *Dask* {.smaller}  
### Why Dask? {.smaller}
:::: {.columns} 

::: {.column width="50%"}  

- It allows you to work beyond the limits of your machine's RAM.
- With Dask, you can run tasks concurrently across multiple cores for faster, more efficient computation. 
- And it scales seamlessly to distributed clusters, making it possible to handle massive, terabyte scale datasets. 
   
::: 

::: {.column width="50%"}  

 ```python
from dask.distributed import Client

# Start Dask cluster

client = Client( n_workers=2, threads_per_worker=1, memory_limit="6GB" )

# Process 18 GB raster with only 6 GB RAM per worker  

raster = rioxarray.open_rasterio(path, chunks={"x": 1024, "y": 1024})   
result = raster.mean().compute()   
# Dask handles chunking and parallelism

```         

:::   


::::
  
## Critical design decisions  {.smaller}
### 1. Harmonizing heterogeneous data 
- One critical step in the workflow was to harmonize the data given that it came from different sources. So I had to clip some files into the region of interest, resample to ensure same resolution, and matching the CRS.  
- To do that, I put the data into three folders, folder_1 for the data that needed downsampling, folder_2 for those that needed upsampling, and folder_3 for those that were in the required resolution and CRS. 

## 
### 2. The chunking challenge {.smaller} 
- Chunking directly impacts: 
  - Memory per worker. 
  - Number of parallel tasks. 
  - Computation efficiency. 

For instance, this code...  
```python
raster = rioxarray.open_rasterio(path, chunks={"x": 1024, "y": 1024})
```         
creates:   

```
    
~260 blocks of 1024×1024 pixels each 
Each chunk: ~4 MB (for float32 data)   

```         

##
### Chunk size
:::: {.columns} 

::: {.column width="50%"}  

::: {.callout-warning}
## Too small chunks 
- Massive task graph    
- Scheduler becomes bottleneck   
- More time managing tasks than processing.   
:::

:::

::: {.column width="50%"} 


::: {.callout-warning} 
## Too large chunks
- Required more memory.  
:::

:::

::::



## Chunking alignment {.smaller}

::: {.callout-important}
When operating on multiple arrays together, they MUST have identical chunk structures. 
:::

```
Raster chunks:              Zone chunks:
┌──────┬──────┐            ┌───┬───┬───┬───┐
│      │      │            │   │   │   │   │
│ 1024 │ 1024 │            ├───┼───┼───┼───┤
│   ×  │   ×  │  ≠         │512│512│512│512│
│ 1024 │ 1024 │            │ × │ × │ × │ × │
│      │      │            │512│512│512│512│
└──────┴──────┘            └───┴───┴───┴───┘

Any operations on this chunks won't work as they don't align!

```
## 3. Rasterizing adminstrative zones 

Create rasterized zone array with chunks EXACTLY matching raster.

  ![](www/rasterize_admin_zones.png)

##
### 4. Understanding Dask graphs {.smaller}
- A Dask graph is Dask's internal presentation of your computation as a directed acyclic graph (DAG)

Task graph in my pipeline: 

```python

LAYER 1: Read Raster (260 tasks) 
├─ read[0,0], read[0,1], ..., read[19,12]

LAYER 2: Rasterize Zones (260 tasks) 
├─ rasterize[0,0] ← depends on read[0,0] 
├─ rasterize[0,1] ← depends on read[0,1] 
└─ ... (258 more)

LAYER 3: Compute Statistics (260 tasks) 
├─ block_stats[0,0] ← depends on read[0,0] AND rasterize[0,0] 
├─ block_stats[0,1] ← depends on read[0,1] AND rasterize[0,1] 
└─ ... (258 more)

LAYER 4: Merge Results (1 task) 
└─ merge_all ← depends on ALL 260 block_stats

Total: 781 tasks

```         

## 5. Reprojection with WarpedVRT {.smaller}
- Before discovering WarpedVRT, reprojection was a pain point, even with chunking.  
- This was potentially caused by chunk misalignment across CRS transformations. 
- WarpedVRT is a virtual raster format that performs on-demand, windowed reprojection.   
- How it works:  

  - It reads and writes data in chunks or directly between files. 
  - This it does by creating a virtual file/view (think of it like a database view) with the desired CRS and resolution. 
  - So when reading the chunks for further processing, we are actually reading from the reprojected raster. 
  
::: {.callout-tip}
You can also use command-line tool like GDAL to reproject your files. 
:::

## 6. Dask configuration
:::: {.columns}

::: {.column width="50%"}

::: {.callout-tip} 
- Depending on your resources, choosing the right configuration determines how fast your pipeline executes, or even, whether it executes.  
- For my case - more workers with less threads worked best. And of course working within available RAM.  
- what to consider - are your operations cpu-bound or I/O bound.  
:::

:::

::: {.column width="50%"}

  ![](www/dask_configuration.png)
:::
::::
## Pipeline summary

![](www/pipeline_summary.png) 

## Key lessons {.smaller}

1.Chunking is fundamental. 

- Right size balances parallelism and overhead.
- Alignment across arrays is critical.

2.Lazy evaluation is powerful.  

- Build entire pipeline before execution.
- Single compute() for complex workflows.
- WarpedVRT enables lazy reprojection.

3.The journey is iterative.  

- Each error teaches something.
- Debugging deepens understanding.
- Build incrementally.

## Further reading {.smaller}

Xarray - Documentation: https://docs.xarray.dev/ - Tutorial: https://tutorial.xarray.dev/

Rioxarray - Documentation: https://corteva.github.io/rioxarray/ Examples: https://corteva.github.io/rioxarray/stable/examples.html

Dask - Documentation: https://docs.dask.org/ - Best Practices: https://docs.dask.org/en/stable/best-practices.html - Array Documentation: https://docs.dask.org/en/stable/array.html

GDAL Virtual Format (VRT) - VRT Tutorial: https://gdal.org/drivers/raster/vrt.html - WarpedVRT: https://rasterio.readthedocs.io/en/latest/api/rasterio.vrt.html  

## 
::: {.center-xy}

Thank you !       

email: oyogoc@gmail.com     

LinkedIn: Oyogo Clinton    

:::